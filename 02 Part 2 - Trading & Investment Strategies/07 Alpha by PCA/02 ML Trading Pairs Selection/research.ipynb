{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n","<hr>\n","\n","# Introduction\n","\n","This notebook demonstrations a method of using machine learning to discover candidate pairs of stocks for a pairs trading strategy. The process involves dimensionality reduction by principal component analysis, clustering with the OPTICS algorithm, and then several statistical tests.\n","\n","# Step 1: Dimensionality Reduction\n","\n","Before you can cluster assets into groups, we need to assign \"coordinates\" to each asset. The way we'll define the coordinates in this example is by gathering the daily returns of the assets, applying principal component analysis to reduce the matrix of daily returns to just 3 principal components, then finding how much each asset's series of returns contributes to each of the three principal components. The result effectively places each asset into a 3D space."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","import plotly.graph_objects as go\n","\n","qb = QuantBook()\n","\n","# Select a single trading day to use for the analysis.\n","end_date = datetime(2024, 1, 3)\n","\n","# Get a list of all assets that were trading for the selected trading \n","# day. \n","universe_history = qb.universe_history(\n","    qb.universe.etf('IWB'), end_date - timedelta(1), end_date\n",")\n","symbols = [fundamental.symbol for fundamental in universe_history.iloc[0]]\n","\n","# Get the historical daily returns of the selected assets.\n","prices = qb.history(\n","    symbols, end_date - timedelta(3 * 365), end_date, Resolution.DAILY\n",")['close'].unstack(0).dropna(axis=1)\n","daily_returns = prices.pct_change().dropna()\n","\n","# Standardize each series of daily returns.\n","standardized_returns = StandardScaler().fit_transform(daily_returns)\n","\n","# Perform PCA (reduce dataset dimensions)\n","pca = PCA(n_components=3, random_state=0)\n","pca.fit(standardized_returns)\n","\n","factor_exposures = pd.DataFrame(\n","    index=[f\"component_{i}\" for i in range(pca.components_.shape[0])], \n","    columns=daily_returns.columns,\n","    data=pca.components_ \n",").T\n","\n","# Display the results.\n","go.Figure(\n","    [\n","        go.Scatter3d(\n","            x=[row['component_0']],\n","            y=[row['component_1']],\n","            z=[row['component_2']],\n","            mode='markers',\n","            marker=dict(size=3, color='blue'),\n","            text=symbol,\n","            showlegend=False\n","        )\n","        for symbol, row in factor_exposures.iterrows()\n","    ],\n","    dict(\n","        scene=dict(\n","            xaxis_title='X', yaxis_title='Y', zaxis_title='Z', \n","            camera=dict(\n","                eye=dict(x=2, y=1.5, z=1.5)\n","            )\n","        ),\n","        title='3D Representation of Assets<br><sup>The coordinates represent '\n","            + 'the contribution the asset has to each component</sup>', \n","    )\n",").show()\n","\n","# Each row of pca.components_ corresponds to a principal component, and \n","# the values within each row indicate the contribution of the original \n","# features to that principal component.\n","factor_exposures"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Step 2: Clustering\n","Now that the assets are organized into 3D space, you can use a clustering algorithm to place them into groups. Let's use the OPTICS clustering algorithm."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from sklearn.cluster import OPTICS\n","\n","clustering = OPTICS().fit(factor_exposures)\n","\n","# Display the results.\n","group_by_cluster_id = {\n","    cluster_id: factor_exposures.iloc[\n","        [i for i, x in enumerate(clustering.labels_) if x == cluster_id]\n","    ]\n","    for cluster_id in sorted(set(clustering.labels_))\n","}\n","go.Figure(\n","    [\n","        go.Scatter3d(\n","            x=group['component_0'], y=group['component_1'], \n","            z=group['component_2'], mode='markers',\n","            marker=dict(size=3), text=group.index,\n","            name=f\"Group {cluster_id}\" if cluster_id >= 0 else \"Noisy group\",\n","            visible=True if cluster_id >= 0 else 'legendonly'\n","        )\n","        for cluster_id, group in group_by_cluster_id.items()\n","    ],\n","    dict(\n","        scene=dict(\n","            xaxis_title='X', yaxis_title='Y', zaxis_title='Z', \n","            camera=dict(eye=dict(x=2, y=1.5, z=1.5))\n","        ),\n","        title='3D Representation of Assets<br><sup>The coordinates represent '\n","            + 'the contribution the asset has to each component</sup>', \n","    )\n",").show()\n","\n","# Drop noisy samples.\n","labels = clustering.labels_[clustering.labels_ != -1]\n","print(\n","    f\"Out of {len(clustering.labels_)} assets, OPTICS found {len(labels)}\",\n","    f\"non-noisy samples and organzied them into {len(set(labels))} groups.\"\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Step 3: Absolute Rules of Disqualification\n","With the assets seperated into distinct groups, you can now look for candidate pairs within each group. In this example, a pair is selected if it complies with the following four conditions:\n","1. The pair’s constituents are cointegrated.\n","2. The pair’s spread Hurst exponent reveals a mean-reverting character.\n","3. The pair’s spread diverges and converges within convenient periods.\n","4. The pair’s spread reverts to the mean with enough frequency."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from itertools import combinations\n","from statsmodels.tsa.stattools import coint\n","from statsmodels.regression.linear_model import OLS\n","from statsmodels.tools.tools import add_constant\n","from hurst import compute_Hc\n","from arch.unitroot.cointegration import engle_granger\n","\n","pairs_tested = 0\n","test_results_index = [\n","    'Failed at cointegration test', 'Failed at Hurst test', \n","    'Failed at half-life test', 'Failed at crossing the mean'\n","]\n","test_results = pd.DataFrame(\n","    {\"count\" : [0] * len(test_results_index)}, \n","    index=test_results_index\n",")\n","pairs_detected = []\n","\n","# Loop through each cluster.\n","for cluster_id in range(len(set(labels))):\n","    # Select all assets in this cluster.\n","    cluster_symbols = list(\n","        factor_exposures.index[clustering.labels_ == cluster_id]\n","    )\n","\n","    # Loop through all possible pairs in the cluster.\n","    for symbol_a, symbol_b in combinations(cluster_symbols, 2):\n","        pairs_tested += 1\n","\n","        # Test 1: The pair’s constituents are cointegrated.\n","        # 1.1: Run the Engle-Granger tests.\n","        cointegration_test_results = [\n","            engle_granger(prices[symbol_a], prices[symbol_b]),\n","            engle_granger(prices[symbol_b], prices[symbol_a])\n","        ]\n","\n","        # 1.2: Select the test result with the lowest test statistic \n","        # (the more significant one).\n","        cointegration_test_result = sorted(\n","            cointegration_test_results, key=lambda x: x.stat\n","        )[0]\n","\n","        # 1.3: Check the p-value of the Engle-Granger test.\n","        if cointegration_test_result.pvalue > 0.01:\n","            test_results.loc[test_results_index[0], 'count'] += 1\n","            continue  # Test failed: This pair is not cointegrated \n","\n","        # Test 2: The pair’s spread Hurst exponent reveals a \n","        # mean-reverting character.\n","        spread = (\n","            prices[cointegration_test_result.cointegrating_vector.index[0]] \n","            + cointegration_test_result.cointegrating_vector.values[1] \n","            * prices[cointegration_test_result.cointegrating_vector.index[1]]\n","        )\n","        H, _, _ = compute_Hc(spread, kind='price', simplified=False)\n","        if H >= 0.5:\n","            test_results.loc[test_results_index[1], 'count'] += 1\n","            # Test failed: The spread doesn't lean towards \n","            # mean-reverting.\n","            continue\n","        \n","        # Test 3: The pair's spread diverges and converges within \n","        # convenient periods.\n","        lagged_spread = np.roll(spread, 1)\n","        lagged_spread[0] = 0\n","        spread_delta = spread - lagged_spread\n","        spread_delta.iloc[0] = 0\n","        # Run OLS regression to find regression coefficient.\n","        model = OLS(spread_delta, add_constant(lagged_spread))\n","        beta = model.fit().params.iloc[1]\n","        half_life = -np.log(2) / beta\n","        if not (1 < half_life < 252): \n","            test_results.loc[test_results_index[2], 'count'] += 1\n","            # Test failed: Half life not between 1 day and 1 year.\n","            continue \n","\n","        # Test 4: The pair's spread reverts to the mean with enough \n","        # frequency (12+ times per year).\n","        mean_value = spread.mean()\n","        crossings = (\n","            (spread > mean_value) & (spread.shift(1) <= mean_value) | \n","            (spread < mean_value) & (spread.shift(1) >= mean_value)\n","        )\n","        num_crossings = crossings.sum()\n","        if num_crossings < 3 * 12:  # 36 months in 3 years\n","            test_results.loc[test_results_index[3], 'count'] += 1\n","            # Test failed: Spread didn't cross the mean at least 12 \n","            # times per year.\n","            continue\n","\n","        pairs_detected.append(\n","            {\n","                'symbol_a': symbol_a, \n","                'symbol_b': symbol_b, \n","                'spread': spread, \n","                'cointegrating_vector': cointegration_test_result.\\\n","                    cointegrating_vector\n","            }\n","        )\n","\n","test_results['percent_of_tested'] = round(\n","    test_results['count'] / pairs_tested, 4\n",")\n","test_results['cumulative_percent_of_tested'] = \\\n","    test_results['percent_of_tested'].cumsum()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Display Results\n","\n","Let's analyse the search results. Print the number of pairs detected and show some plots for each candidate pair."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","print(f\"Pairs tested: {pairs_tested}\")\n","display(test_results) \n","print(f\"Pairs detected: {len(pairs_detected)}\")\n","\n","num_rows = len(pairs_detected)\n","fig = make_subplots(\n","    rows=num_rows, cols=2, \n","    subplot_titles=num_rows * [\n","        \"Prices of Candidate Pair\", \"Normalized Spread\"\n","    ], \n","    specs=[[{'secondary_y': True}, {}] for _ in range(num_rows)],\n","    vertical_spacing=0.02, horizontal_spacing=0.1\n",")\n","\n","for i, pair in enumerate(pairs_detected):\n","    symbol_a = pair['symbol_a']\n","    symbol_b = pair['symbol_b']\n","    price_a = prices[symbol_a]\n","    price_b = prices[symbol_b]\n","\n","    # Add traces to the left subplot with 2 y-axis labels\n","    fig.add_trace(\n","        go.Scatter(\n","            x=price_a.index, y=price_a, mode='lines', name=f'{symbol_a}', \n","            line={'color': 'blue'}\n","        ),\n","        row=i+1, col=1\n","    )\n","    fig.add_trace(\n","        go.Scatter(\n","            x=price_b.index, y=price_b, mode='lines', name=f'{symbol_b}', \n","            line={'color': 'red'}\n","        ),\n","        row=i+1, col=1,\n","        secondary_y=True\n","    )\n","\n","    # Update y-axis titles for the left subplot\n","    for j in range(2):\n","        fig.update_xaxes(title_text=\"Time\", row=i+1, col=j+1)\n","    fig.update_yaxes(\n","        title_text=f'Price({symbol_a})', row=i+1, col=1, secondary_y=False\n","    )\n","    fig.update_yaxes(\n","        title_text=f'Price({symbol_b})', row=i+1, col=1, secondary_y=True, \n","        showgrid=False\n","    )\n","    fig.update_yaxes(\n","        title_text=f'Normalized Spread', row=i+1, col=2, secondary_y=False\n","    )\n","\n","    normalized_spread = pd.Series(\n","        StandardScaler().fit_transform(\n","            pair['spread'].values.reshape(-1, 1)\n","        ).flatten(),\n","        index=pair['spread'].index\n","    )\n","\n","    # Add trace to the right subplot\n","    fig.add_trace(\n","        go.Scatter(\n","            x=normalized_spread.index, y=normalized_spread, mode='lines', \n","            name='Right Plot', line={'color': 'green'}\n","        ),\n","        row=i+1, col=2\n","    )\n","\n","# Update layout\n","fig.update_layout(\n","    title_text=\"Pairs Detected<br><sup>Each row shows prices of the assets in \"\n","        + \"the pair and their normalized spread</sup>\", \n","    height=380*num_rows, showlegend=False\n",")\n","\n","# Show the plot\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Foundation-Py-Default","language":"python","name":"python3"},"vscode":{"interpreter":{"hash":"3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"}}},"nbformat":4,"nbformat_minor":2}
