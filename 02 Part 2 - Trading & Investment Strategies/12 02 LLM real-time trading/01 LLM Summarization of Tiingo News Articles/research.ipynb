{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n","<hr>\n","\n","# Introduction\n","This notebook demonstrates how to create hourly sentiment scores for Tesla with ChatGPT based solely on Tesla's news releases. The last cell of the notebook saves the sentiment scores into the Object Store so that `main.py` can use them as a factor in a trading strategy.\n","\n","# Get Asset(s)\n","\n","In this example, subscribe to Tesla."]},{"cell_type":"code","execution_count":1,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# Create a QuantBook.\n","qb = QuantBook()\n","\n","# Subscribe to an asset.\n","symbol = qb.add_equity(\"TSLA\").symbol"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Get News Articles of Asset(s)\n","\n","Get all news articles for Tesla that were released between November 2023 and March 2024."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Subscribe to Tiingo news.\n","dataset_symbols = qb.add_data(TiingoNews, symbol).symbol\n","\n","# Get news articles.\n","news_articles = qb.history[TiingoNews](\n","    dataset_symbols, datetime(2023, 11, 1), datetime(2024, 3, 1), \n","    Resolution.DAILY\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Group Articles By Date and Visualize Article Counts\n","In this example, we want to produce sentiment scores for each hour. To start, group the news articles by date. Some articles can be duplicates. Let's try to drop them during this process."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import plotly.graph_objects as go\n","\n","# Group articles by date and remove duplicates.\n","articles_by_date = {}\n","deduplicated_articles_by_date = {}\n","duplicates_by_date = {}\n","past_titles = []\n","for article in news_articles:\n","    date = article.end_time.date()\n","    if date not in articles_by_date:\n","        articles_by_date[date] = []\n","    articles_by_date[date].append(article)\n","    \n","    # Drop this article if the title is the same as one of the last 100 \n","    # article titles.\n","    if date not in duplicates_by_date:\n","        duplicates_by_date[date] = 0\n","    if article.title in past_titles:\n","        duplicates_by_date[date] += 1\n","        continue\n","    past_titles.append(article.title)\n","    past_titles = past_titles[-100:]\n","    if date not in deduplicated_articles_by_date:\n","        deduplicated_articles_by_date[date] = []\n","    deduplicated_articles_by_date[date].append(article)\n","\n","def plot_article_counts(data, duplicates_by_date=None):\n","    article_counts = pd.DataFrame()\n","    for date, articles in data.items():\n","        article_counts.loc[date, 'count'] = len(articles)\n","    article_counts['cumulative_count'] = article_counts['count'].cumsum()\n","    article_counts.index = pd.to_datetime(article_counts.index)\n","    \n","    series = [\n","        go.Scatter(\n","            x=article_counts.index, y=article_counts['count'], \n","            name='Count'\n","        ),\n","        go.Scatter(\n","            x=article_counts.index, y=article_counts['cumulative_count'], \n","            name='Cumulative Count', yaxis='y2'\n","        )\n","    ]\n","    if duplicates_by_date:\n","        series.append(\n","            go.Scatter(\n","                x=list(duplicates_by_date.keys()), \n","                y=list(duplicates_by_date.values()), \n","                name='Duplicates'\n","            )\n","        )\n","\n","    go.Figure(\n","        series,\n","        dict(\n","            title='Article Counts',\n","            yaxis=dict(title='Count', side='left', showgrid=True),\n","            yaxis2=dict(\n","                title='Cumulative Count', overlaying='y', side='right', \n","                showgrid=False\n","            ),\n","            xaxis=dict(title='Date'),\n","            showlegend=True\n","        )\n","    ).show()\n","\n","# Plot article counts.\n","plot_article_counts(articles_by_date, duplicates_by_date)\n","\n","print(f\"{sum(duplicates_by_date.values())} duplicates found\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Group Articles By Hour and Visualize Article Counts\n","Now let's group the articles by each hour. This grouping reduces the number of calls we need to make to the OpenAI API and enables us get a sentiment score for each hour with all of the articles that were released during that hour."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["aggregated_articles_by_timestamp = {}\n","\n","# Iterate through each day.\n","for date, articles in deduplicated_articles_by_date.items():\n","    # Group this day's articles into hourly buckets.\n","    articles_by_hour = {} \n","    for article in articles:\n","        # The keys represent the start of the hour, not the end.\n","        hour = article.end_time.hour\n","        if hour not in articles_by_hour:\n","            articles_by_hour[hour] = []\n","        articles_by_hour[hour].append(article)\n","\n","    for hour, articles in articles_by_hour.items():\n","        timestamp = datetime(date.year, date.month, date.day, hour)\n","        aggregated_articles_by_timestamp[timestamp] = articles\n","\n","plot_article_counts(aggregated_articles_by_timestamp)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Get Hourly Sentiment Values from OpenAI\n","\n","In the following code block, we pass all of the articles within each hour to ChatGPT and ask it to provide hourly sentiment scores. The prompt we use is:\n","> Review the news titles and descriptions above and then create an aggregated sentiment score which represents the emotional positivity towards TSLA after seeing all of the news articles. -10 represents extreme negative sentiment, +10 represents extreme positive sentiment, and 0 represents neutral sentiment. Reply ONLY with the numerical value in JSON format. For example, `{ \"sentiment-score\": 0 }\"\n","\n","We then parse the response and save all the data into the Object Store so that we can import it into the trading algorithm."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from openai import OpenAI\n","\n","client = OpenAI(api_key=\"<your_api_key>\")\n","\n","# Iterate through each day.\n","for date, articles in deduplicated_articles_by_date.items():\n","    print(date)\n","    # Group this day's articles into hourly buckets.\n","    articles_by_hour = {}\n","    for article in articles:\n","        hour = article.end_time.hour\n","        if hour not in articles_by_hour:\n","            articles_by_hour[hour] = []\n","        articles_by_hour[hour].append(article)\n","    \n","    # Create a series to hold the sentiment scores for the hours in this\n","    # day.\n","    sentiment_by_hour = pd.DataFrame(dtype=float)\n","    for hour, articles in articles_by_hour.items():\n","        # Create a prompt for OpenAI.\n","        prompt = \"\"\n","        for i, article in enumerate(articles):\n","            prompt += (\n","                f\"Article {i+1} title: {article.Title}\\n\"\n","                + f\"Article {i+1} description: {article.Description}\\n\\n\"\n","            )\n","        prompt += (\n","            \"Review the news titles and descriptions above and then create an \"\n","            + \"aggregated sentiment score which represents the emotional \"\n","            + \"positivity towards TSLA after seeing all of the news articles. \"\n","            + \"-10 represents extreme negative sentiment, +10 represents \"\n","            + \"extreme positive sentiment, and 0 represents neutral sentiment.\"\n","            + \" Reply ONLY with the numerical value in JSON format. For \"\n","            + 'example, `{ \"sentiment-score\": 0 }`'\n","        )\n","        \n","        # Call the OpenAI API to get the sentiment.\n","        chat_completion = client.chat.completions.create(\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            model=\"gpt-4\"\n","        )\n","        sentiment = json.loads(\n","            chat_completion.choices[0].message.content\n","        )['sentiment-score']\n","\n","        # Save the factors.\n","        sentiment_by_hour.loc[hour, 'sentiment'] = sentiment\n","        sentiment_by_hour.loc[hour, 'volume'] = len(articles)\n","    \n","    # Save the dataset file to the Object Store.\n","    file_path = qb.object_store.get_file_path(\n","        f\"tiingo-{date.strftime('%Y-%m-%d')}.csv\"\n","    )\n","    sentiment_by_hour.to_csv(file_path)"]}],"metadata":{"kernelspec":{"display_name":"Foundation-Py-Default","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":2}
