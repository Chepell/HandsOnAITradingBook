{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n","<hr>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Introduction\n","This notebook illustrates how applying various pre-processing techniques to factors values can impact the accuracy of a machine learning model. The model used here is a random forest from the `lightgbm` Python package.\n","\n","# Part 1: Get Raw Data\n","Gather some raw data so you can train and test the model. Start with the producing the label, which is 1 if the future weekly return for SPY market index from open to open is positive, otherwise it's 0."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import plotly.graph_objects as go\n","\n","# Create a QuantBook.\n","qb = QuantBook()\n","# Add the asset.\n","symbol = qb.add_equity(\"SPY\", Resolution.DAILY).symbol\n","# Get the asset history.\n","history = qb.history(symbol, datetime(2000, 1, 1), datetime(2024, 1, 1))\n","# Calculate the labels.\n","label = history.loc[symbol]['open'].pct_change(5).shift(-5).dropna().apply(\n","    lambda x: int(x > 0)\n",")\n","# Show the result.\n","go.Figure(\n","    go.Scatter(\n","        x=label.index, y=history.loc[symbol]['open'], mode='markers', \n","        marker=dict(\n","            color=['blue' if x else 'red' for x in label.values], size=3\n","        )\n","    ),\n","    dict(\n","        title=\"Label Distribution<br><sup>Labels change frequently \"\n","            + \"as a result of market volatility.</sup>\", \n","        xaxis_title=\"Date\", yaxis_title=\"Price\", \n","        xaxis={'range': [label.index[0], label.index[-1]]}\n","    )\n",").show()\n","print(label)\n","print(label.value_counts())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, define the factors you'll input into the model to predict the label. For demonstration purposes, use random factors."]},{"cell_type":"code","execution_count":46,"metadata":{"pycharm":{"name":"#%%\n"}},"outputs":[],"source":["np.random.seed(2)\n","num_factors = 4\n","num_samples = len(label)\n","factors = np.random.rand(num_samples, num_factors)\n","# Make one of the factors non-stationary.\n","factors[:, -1] = factors[:, -1].cumsum()\n","factors"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Part 2: Test Model Accuracy Using Raw Factors\n","The next code block defines a method to train and test the model. The method below uses 75% of the data to train it and leaves 25% of the data for the out-of-sample test. Let's see how the model performs with just the raw factor values. The number that the method returns represents the percentage of samples in the out-of-sample dataset that the model predicted the correct label. The method also displays a line plot showing the probability that the model predicts for each possible label (0 or 1) for each sample in the tst set. The factors are random, but there are more labels of class 1 than class 0, so we should expect the model to give a greater probability to class 1 for each prediction and we should expect an accuracy slightly greater than 50%."]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["import lightgbm as lgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def oos_accuracy(factors, label):\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        factors, label, test_size=0.25, shuffle=False\n","    )\n","    model = lgb.train(\n","        {\n","            'seed' : 1234, \n","            'verbose': -1, \n","            'boosting_type': 'rf', \n","            'feature_fraction': 0.8, \n","            'objective': 'multiclass', \n","            'num_class': 2, \n","            'bagging_freq': 5, \n","            'bagging_fraction': 0.8,\n","            'is_unbalanced': True\n","        }, \n","        train_set=lgb.Dataset(\n","            data=X_train, label=y_train, free_raw_data=True\n","        ).construct()\n","    )\n","    predictions = model.predict(X_test)\n","    x = list(range(len(predictions)))\n","    go.Figure(\n","        [\n","            go.Scatter(x=x, y=predictions[:, 0], name=0),\n","            go.Scatter(x=x, y=predictions[:, 1], name=1)\n","        ],\n","        dict(\n","            title=\"Probability of Each Label<br><sup>Class 1 gets a greater \"\n","                + \"probability because the SPY has an upward bias</sup>\", \n","            xaxis_title=\"Date\", yaxis_title=\"Probability\"\n","        )\n","    ).show()\n","    y_hat = predictions.argmax(axis=1)\n","    print(f\"Accuracy: {round(accuracy_score(y_hat, y_test), 4)}\")\n","\n","oos_accuracy(factors, label)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Part 3: Test Model Accuracy Using Stationary Factors\n","\n","Lopez de Prado explains that \"supervised learning algorithms typically require stationary features\" (2018, p. 76). Let's perform an Augmented Dickeyâ€“Fuller test to see if our factors are stationary at the 95% confidence level.\n","\n","### Test Factor Stationarity"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["from statsmodels.tsa.stattools import adfuller\n","\n","for factor_idx in range(num_factors):\n","    factor = factors[:, factor_idx]\n","    test_results = adfuller(factor, maxlag=1, regression='c', autolag=None)\n","    # Check the p-value.\n","    output = \"Stationary\" if test_results[1] <= 0.05 else \"Not stationary\"\n","    print(f\"Factor {factor_idx}: {output}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Adjust Factor Values to Achieve Stationarity\n","If the raw factors aren't stationary, you can transform them to make them stationary. Lopez de Prado mentions that \"virutally all finance papers attempt to recover stationarity by applying an integer differentiation. . ., which means that most studies have over-differentiated the series, that is, they have removed much more memory than was necessary to statisfy standard econometric assumptions\" (2018, p. 76). To avoid over-differentiating the factors, you can use Lopez de Prado's fractional differentiation technique. The following code is from Lopez de Prado (2018, pp. 79-84):"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["def get_weights_ffd(d, thres):\n","    '''\n","    Computing the weights for differentiating the series with fixed \n","    window size\n","    \n","        Parameters:\n","            d (float): differentiating factor\n","            thres (float): threshold for cutting off weights\n","            \n","        Returns:\n","            w (np.ndarray): array contatining weights\n","    '''\n","    w, k = [1.0], 1\n","    while True:\n","        w_ = -w[-1] / k * (d - k + 1)\n","        if abs(w_) < thres:\n","            break\n","        w.append(w_)\n","        k += 1\n","    w = np.array(w[::-1]).reshape(-1, 1)\n","    return w\n","\n","def frac_diff_ffd(series, d, thres=1e-5):\n","    '''\n","    Fractional differentiation with constant width window\n","    Note 1: thres determines the cut-off weight for the window\n","    Note 2: d can be any positive fractional, not necessarily bounded \n","    [0,1]\n","    \n","        Parameters:\n","            series (pd.DataFrame): dataframe with time series\n","            d (float): differentiating factor\n","            thres (float): threshold for cutting off weights\n","        \n","        Returns:\n","            df (pd.DataFrame): dataframe with differentiated series\n","    '''\n","    w = get_weights_ffd(d, thres)\n","    width = len(w) - 1\n","\n","    df = {}\n","    for name in series.columns:\n","        series_f = series[[name]].ffill().dropna()\n","        df_ = pd.Series(index=np.arange(series.shape[0]), dtype=object)\n","        for iloc1 in range(width, series_f.shape[0]):\n","            loc0, loc1 = series_f.index[iloc1 - width], series_f.index[iloc1]\n","            if not np.isfinite(series.loc[loc1, name]):\n","                continue    # exclude NAs\n","            df_[loc1] = np.dot(w.T, series_f.loc[loc0:loc1])[0, 0]\n","        df[name] = df_.dropna().copy(deep=True)\n","    df = pd.concat(df, axis=1)\n","    return df\n","\n","def ffd(process, thres=0.01):\n","    '''\n","    Finding the minimum differentiating factor that passes the ADF test\n","    \n","        Parameters:\n","            process (np.ndarray): array with random process values\n","            apply_constant_width (bool): flag that shows whether to use \n","             constant width window (if True) or increasing width window \n","             (if False)\n","            thres (float): threshold for cutting off weights\n","    '''    \n","    for d in np.linspace(0, 1, 11):\n","        process_diff = frac_diff_ffd(pd.DataFrame(process), d, thres)\n","        test_results = adfuller(\n","            process_diff[process.name], maxlag=1, regression='c', autolag=None\n","        )\n","        if test_results[1] <= 0.05:\n","            break\n","    return process_diff[process.name]\n","\n","stationary_factors = pd.DataFrame()\n","for factor_idx in range(num_factors):\n","    stationary_factors[factor_idx] = ffd(\n","        pd.Series(factors[:, factor_idx], name=factor_idx)\n","    )\n","stationary_factors"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Test Accuracy\n","Let's now test the out-of-sample accuracy of the model when using the stationary factors."]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["oos_accuracy(stationary_factors.values, label)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Part 4: Test Model Accuracy Using Standardized Factors"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Another common preproccessing technique is standardization, which transforms the factor values to be normally distributed. Let's try it.\n","\n","### Standardize Factors Values\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","standardized_factors = scaler.fit_transform(stationary_factors)\n","standardized_factors"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Test Accuracy\n","Now that you have the standardized factors, use them to test the model's out-of-sample accuracy."]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["oos_accuracy(standardized_factors, label)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Part 5: Test Model Accuracy Using Principal Components\n","Principal component analysis (PCA) is another common preprocessing technique that can reduce the dimensionality of the factors. PCA performs best when the factors are on the same scale ([reference](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)), so in this case, perform PCA on the standardized factors."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Perform PCA"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(random_state=0)\n","principal_components = pca.fit_transform(standardized_factors[1:, :])\n","principal_components"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Test Accuracy"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["oos_accuracy(principal_components, label.iloc[1:])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Foundation-Py-Default","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":2}
